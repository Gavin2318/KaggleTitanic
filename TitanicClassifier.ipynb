{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Ready!\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "from numpy.random import random_integers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from scipy.stats import pointbiserialr, spearmanr\n",
    "%matplotlib inline\n",
    "\n",
    "print('Libraries Ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load data \n",
    "path = '../input/'\n",
    "trainRawData = pd.read_csv(path+'train.csv')\n",
    "testRawData = pd.read_csv(path+'test.csv')\n",
    "# print len(trainRawData)\n",
    "# print len(testRawData)\n",
    "# trainRawData.head()\n",
    "# testRawData.head()\n",
    "fullRawData = pd.concat([trainRawData, testRawData])\n",
    "# print len(fullRawData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.py:2698: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fullData = fullRawData\n",
    "fullData.index = fullData.PassengerId\n",
    "\n",
    "#1. Get the title from the names\n",
    "Title_Dictionary = {\n",
    "                    \"Capt\":       \"Officer\",\n",
    "                    \"Col\":        \"Officer\",\n",
    "                    \"Major\":      \"Officer\",\n",
    "                    \"Jonkheer\":   \"Royalty\",\n",
    "                    \"Don\":        \"Royalty\",\n",
    "                    \"Sir\" :       \"Royalty\",\n",
    "                    \"Dr\":         \"Officer\",\n",
    "                    \"Rev\":        \"Officer\",\n",
    "                    \"the Countess\":\"Royalty\",\n",
    "                    \"Dona\":       \"Royalty\",\n",
    "                    \"Mme\":        \"Mrs\",\n",
    "                    \"Mlle\":       \"Miss\",\n",
    "                    \"Ms\":         \"Mrs\",\n",
    "                    \"Mr\" :        \"Mr\",\n",
    "                    \"Mrs\" :       \"Mrs\",\n",
    "                    \"Miss\" :      \"Miss\",\n",
    "                    \"Master\" :    \"Master\",\n",
    "                    \"Lady\" :      \"Royalty\"\n",
    "                    } \n",
    "\n",
    "fullData['Title'] = fullData['Name'].apply(lambda x: Title_Dictionary[x.split(',')[1].split('.')[0].strip()])\n",
    "\n",
    "\n",
    "#2. Create new variable familyType\n",
    "fullData['fsize'] = fullData['Parch'] + fullData['SibSp'] + 1\n",
    "def familyType( fsize ):\n",
    "    if(fsize == 1):\n",
    "        return 'singleton'\n",
    "    elif(fsize>1 & fsize<5):\n",
    "        return 'small'\n",
    "    elif(fsize >4 ):\n",
    "        return 'large'\n",
    "familyType = fullData['fsize'].map(familyType)\n",
    "fullData['familyType'] = familyType\n",
    "\n",
    "\n",
    "#3. Add in the median age based on the Title, Pclass and Sex of each passenger\n",
    "mask_Age = fullData.Age.notnull()\n",
    "Age_Sex_Title_Pclass = fullData.loc[mask_Age, [\"Age\", \"Title\", \"Sex\", \"Pclass\"]]\n",
    "Filler_Ages = Age_Sex_Title_Pclass.groupby(by = [\"Title\", \"Pclass\", \"Sex\"]).median()\n",
    "Filler_Ages = Filler_Ages.Age.unstack(level = -1).unstack(level = -1)\n",
    "mask_Age = fullData.Age.isnull()\n",
    "Age_Sex_Title_Pclass_missing = fullData.loc[mask_Age, [\"Title\", \"Sex\", \"Pclass\"]]\n",
    "def Age_filler(row):\n",
    "    if row.Sex == \"female\":\n",
    "        age = Filler_Ages.female.loc[row[\"Title\"], row[\"Pclass\"]]\n",
    "        return age\n",
    "    \n",
    "    elif row.Sex == \"male\":\n",
    "        age = Filler_Ages.male.loc[row[\"Title\"], row[\"Pclass\"]]\n",
    "        return age\n",
    "Age_Sex_Title_Pclass_missing[\"Age\"]  = Age_Sex_Title_Pclass_missing.apply(Age_filler, axis = 1)\n",
    "ages = pd.concat([Age_Sex_Title_Pclass[\"Age\"], Age_Sex_Title_Pclass_missing[\"Age\"]])\n",
    "fullData['Age'] = ages\n",
    "\n",
    "#4. Replacing embarked missing value\n",
    "mask_Embarked = fullData.Embarked.isnull()\n",
    "d = fullData.loc[mask_Embarked]\n",
    "fullData.loc[mask_Embarked].Embarked = 'C'\n",
    "#Replacing fare missing value\n",
    "fullData['Fare']=fullData['Fare'].fillna(value=fullData.Fare.mean())\n",
    "\n",
    "#5. Drop unused attributes\n",
    "fullData = fullData.drop(['Ticket', 'Cabin','fsize','Name','PassengerId'], axis=1)\n",
    "\n",
    "#6. Convert the categorial vaiables to dummy variables\n",
    "dummies_Sex=pd.get_dummies(fullData['Sex'],prefix='Sex')\n",
    "dummies_Embarked = pd.get_dummies(fullData['Embarked'], prefix= 'Embarked') \n",
    "dummies_Pclass = pd.get_dummies(fullData['Pclass'], prefix= 'Pclass')\n",
    "dummies_Title = pd.get_dummies(fullData['Title'], prefix= 'Title')\n",
    "dummies_FamilyType = pd.get_dummies(fullData['familyType'], prefix='familyType')\n",
    "fullData = pd.concat([fullData, dummies_Sex, dummies_Embarked, dummies_Pclass, dummies_Title, dummies_FamilyType], axis=1)\n",
    "fullData = fullData.drop(['Sex','Embarked','Pclass','Title','familyType'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Title_Mr', 'Sex_female', 'Sex_male', 'Title_Mrs', 'Title_Miss',\n",
       "       'Pclass_3', 'Pclass_1', 'Fare', 'familyType_singleton',\n",
       "       'familyType_small'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = fullData[:891]\n",
    " \n",
    "columns = train.columns.values\n",
    "param=[]\n",
    "correlation=[]\n",
    "abs_corr=[]\n",
    "\n",
    "for c in columns:\n",
    "    #Check if binary or continuous\n",
    "    if len(train[c].unique())<=2:\n",
    "        corr = spearmanr(train['Survived'],train[c])[0]\n",
    "    else:\n",
    "        corr = pointbiserialr(train['Survived'],train[c])[0]\n",
    "    param.append(c)\n",
    "    correlation.append(corr)\n",
    "    abs_corr.append(abs(corr))\n",
    "\n",
    "#Create dataframe for visualization\n",
    "param_df=pd.DataFrame({'correlation':correlation,'parameter':param, 'abs_corr':abs_corr})\n",
    "#Sort by absolute correlation\n",
    "param_df=param_df.sort_values(by=['abs_corr'], ascending=False)\n",
    "#Set parameter name as index\n",
    "param_df=param_df.set_index('parameter')\n",
    "\n",
    "best_features=param_df.index[1:10+1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = train[best_features].values\n",
    "Y = train.loc[:, 'Survived'].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,  \n",
    "                                                    train_size=0.75,  \n",
    "                                                    test_size=0.25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Output processed data\n",
    "features = np.append(best_features, 'Survived')\n",
    "processedTrain = train[features]\n",
    "new_columns = processedTrain.columns.values \n",
    "new_columns[10] = 'class' \n",
    "processedTrain.columns = new_columns\n",
    "processedTrain.head()\n",
    "processedTrain.to_csv('processedTrain.csv',index = False, header= True)\n",
    "test=fullData[891:]\n",
    "test[best_features].to_csv('processedTest.csv',index = False, header= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.831853634252\n"
     ]
    }
   ],
   "source": [
    "#Tuning using TPOT \n",
    "#http://www.kdnuggets.com/2016/05/tpot-python-automating-data-science.html\n",
    "from sklearn.cross_validation import train_test_split  \n",
    "from tpot import TPOT \n",
    "\n",
    "my_tpot = TPOT(generations=10)  \n",
    "my_tpot.fit(X_train, Y_train)  \n",
    "print(my_tpot.score(X_test, Y_test)) \n",
    "\n",
    "my_tpot.export('exported_Classifier.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPOT exported script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'class' in the data file\n",
    "tpot_data = pd.read_csv('processedTrain.csv', sep=',')\n",
    "training_indices, testing_indices = train_test_split(tpot_data.index, stratify = tpot_data['class'].values, train_size=0.75, test_size=0.25)\n",
    "\n",
    "\n",
    "result1 = tpot_data.copy()\n",
    "\n",
    "# Perform classification with a logistic regression classifier\n",
    "lrc1 = LogisticRegression(C=0.01)\n",
    "lrc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)\n",
    "result1['lrc1-classification'] = lrc1.predict(result1.drop('class', axis=1).values)\n",
    "\n",
    "# Use Scikit-learn's SelectKBest for feature selection\n",
    "training_features = result1.loc[training_indices].drop('class', axis=1)\n",
    "training_class_vals = result1.loc[training_indices, 'class'].values\n",
    "\n",
    "if len(training_features.columns.values) == 0:\n",
    "    result2 = result1.copy()\n",
    "else:\n",
    "    selector = SelectKBest(f_classif, k=min(24, len(training_features.columns)))\n",
    "    selector.fit(training_features.values, training_class_vals)\n",
    "    mask = selector.get_support(True)\n",
    "    mask_cols = list(training_features.iloc[:, mask].columns) + ['class']\n",
    "    result2 = result1[mask_cols]\n",
    "\n",
    "# Perform classification with an eXtreme gradient boosting classifier\n",
    "xgbc3 = XGBClassifier(learning_rate=0.1, n_estimators=77, max_depth=4320)\n",
    "xgbc3.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)\n",
    "result3 = result2.copy()\n",
    "result3['xgbc3-classification'] = xgbc3.predict(result3.drop('class', axis=1).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Predict Value\n",
    "testData = pd.read_csv('processedTest.csv', sep=',')\n",
    "testData.head()\n",
    "testData['lrc1-classification'] = lrc1.predict(testData.values)\n",
    "testData['Survived'] = xgbc3.predict(testData.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Write prediction result to file\n",
    "testRawData['Survived'] = testData['Survived']\n",
    "testRawData.head()\n",
    "result=testRawData[['PassengerId','Survived']]\n",
    "result.to_csv('TPOTprediction.csv', index= False, header='True')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
